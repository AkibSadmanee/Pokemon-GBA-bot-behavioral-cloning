{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import numpy.random\n",
    "import matplotlib.pyplot as plt\n",
    "import os, sys, socket\n",
    "\n",
    "import tensorflow as tf\n",
    "from PIL import Image\n",
    "\n",
    "import keras\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, Dropout, Activation, Input, Conv2D, Flatten\n",
    "from keras.layers import Concatenate\n",
    "from keras.utils import np_utils\n",
    "from keras.optimizers import SGD, Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_ds = keras.utils.image_dataset_from_directory(\n",
    "#     directory='train_bw/',\n",
    "#     labels='inferred',\n",
    "#     label_mode='categorical',\n",
    "#     color_mode='grayscale',\n",
    "#     # batch_size=100,\n",
    "#     image_size=(800, 600))\n",
    "\n",
    "# validation_ds = keras.utils.image_dataset_from_directory(\n",
    "#     directory='test_bw',\n",
    "#     labels='inferred',\n",
    "#     label_mode='categorical',\n",
    "#     color_mode='grayscale',\n",
    "#     # batch_size=75,\n",
    "#     image_size=(800, 600))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_generator(type=\"train\", batchsize=100):\n",
    "    if type == \"train\":\n",
    "        print(\"Loading Training data...\")\n",
    "        filepath = os.path.join(\"./data\", \"train_bw\")\n",
    "        rng = 299\n",
    "\n",
    "    elif type == \"valid\":\n",
    "        print(\"Loading Validation data...\")\n",
    "        filepath = os.path.join(\"./data\", \"test_bw\")\n",
    "        rng = 14\n",
    "\n",
    "    for _ in range(70):\n",
    "        for i, folder in enumerate(['down', 'up', 'left', 'right', 'x']):\n",
    "            cur_filepath = os.path.join(filepath, folder)\n",
    "            X1 = []\n",
    "            X2 = []\n",
    "            target = []\n",
    "\n",
    "            for j in range(rng):\n",
    "                X1.append(np.expand_dims(np.asarray(Image.open(os.path.join(cur_filepath, f'{folder}_{j}.jpg'))), axis=2))\n",
    "                X2.append(np.expand_dims(np.asarray(Image.open(os.path.join(cur_filepath, f'{folder}_{j+1}.jpg'))), axis=2))\n",
    "                target.append(np_utils.to_categorical(i, 5))\n",
    "                if len(X1) == batchsize:\n",
    "                    yield {'input1':np.asarray(X1) , 'input2':np.asarray(X2)},  {'out':np.asarray(target)}\n",
    "                    X1 = []\n",
    "                    X2 = []\n",
    "                    target = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = data_generator(\"train\", 100)\n",
    "data_valid = data_generator(\"valid\", 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Training data...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(100, 600, 800, 1)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train.__next__()[0]['input1'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input1 (InputLayer)            [(None, 800, 600, 1  0           []                               \n",
      "                                )]                                                                \n",
      "                                                                                                  \n",
      " input2 (InputLayer)            [(None, 800, 600, 1  0           []                               \n",
      "                                )]                                                                \n",
      "                                                                                                  \n",
      " conv2d (Conv2D)                (None, 267, 200, 64  1664        ['input1[0][0]',                 \n",
      "                                )                                 'input2[0][0]']                 \n",
      "                                                                                                  \n",
      " flatten (Flatten)              (None, 3417600)      0           ['conv2d[0][0]']                 \n",
      "                                                                                                  \n",
      " flatten_1 (Flatten)            (None, 3417600)      0           ['conv2d[1][0]']                 \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 256)          874905856   ['flatten[0][0]',                \n",
      "                                                                  'flatten_1[0][0]']              \n",
      "                                                                                                  \n",
      " concatenate (Concatenate)      (None, 512)          0           ['dense[0][0]',                  \n",
      "                                                                  'dense[1][0]']                  \n",
      "                                                                                                  \n",
      " dense_1 (Dense)                (None, 1024)         525312      ['concatenate[0][0]']            \n",
      "                                                                                                  \n",
      " dense_2 (Dense)                (None, 512)          524800      ['dense_1[0][0]']                \n",
      "                                                                                                  \n",
      " dense_3 (Dense)                (None, 128)          65664       ['dense_2[0][0]']                \n",
      "                                                                                                  \n",
      " dense_4 (Dense)                (None, 64)           8256        ['dense_3[0][0]']                \n",
      "                                                                                                  \n",
      " out (Dense)                    (None, 5)            325         ['dense_4[0][0]']                \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 876,031,877\n",
      "Trainable params: 876,031,877\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Input data.\n",
    "input1   = Input(shape=(800,600, 1), dtype='float32', name='input1')\n",
    "input2   = Input(shape=(800,600, 1), dtype='float32', name='input2')\n",
    "\n",
    "nb_filter      = 64   \n",
    "nb_row, nb_col = 5,5  \n",
    "strides        = (3,3)\n",
    "conv  =  Conv2D(nb_filter, (nb_row, nb_col), activation='relu', padding='same', strides=strides)\n",
    "x1    = conv(input1) \n",
    "x2    = conv(input2)\n",
    "\n",
    "# Flatten data: transform from (28,28) to (784,)\n",
    "x1 = Flatten()(x1)\n",
    "x2 = Flatten()(x2)\n",
    "\n",
    "# Layer two: Single fully-connected layer applied to each input independently, with shared weights.\n",
    "layer = Dense(256, activation='relu')\n",
    "x1 = layer(x1)\n",
    "x2 = layer(x2)\n",
    "x = Concatenate()([x1, x2])\n",
    "\n",
    "# More dense layers then output.\n",
    "x   = Dense(1024, activation='relu')(x)\n",
    "x   = Dense(512, activation='relu')(x)\n",
    "x   = Dense(128, activation='relu')(x)\n",
    "x   = Dense(64, activation='relu')(x)\n",
    "out = Dense(5, activation='softmax', name='out')(x)\n",
    "model = Model(inputs=[input1, input2], outputs=[out])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
    "\n",
    "checkpoint = ModelCheckpoint(\n",
    "    os.path.join(\"./models\", \"model.h5\"),\n",
    "    monitor='val_loss',\n",
    "    mode='min',\n",
    "    save_best_only=True,\n",
    "    verbose=1)\n",
    "\n",
    "# earlystop = EarlyStopping(\n",
    "#     monitor='val_loss',\n",
    "#     min_delta=0,\n",
    "#     patience=20,\n",
    "#     verbose=1, restore_best_weights=True)\n",
    "\n",
    "learning_rate_reduction = ReduceLROnPlateau(monitor='val_accuracy',\n",
    "                                            patience=5,\n",
    "                                            verbose=1,\n",
    "                                            factor=0.2,\n",
    "                                            min_lr=0.0001)\n",
    "\n",
    "callbacks = [checkpoint, learning_rate_reduction]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\sadma\\miniconda3\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "optimizer = Adam(lr=0.001, beta_1=0.99, beta_2=0.999, epsilon=1e-08) # Optimization hyperparameters.\n",
    "\n",
    "model.compile(optimizer=optimizer,\n",
    "              loss={'out':'categorical_crossentropy'},\n",
    "              loss_weights={'out': 1.0}, # \n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/70\n",
      "15/15 [==============================] - ETA: 0s - loss: 22720.6641 - accuracy: 0.1333Loading Validation data...\n",
      "\n",
      "Epoch 1: val_loss improved from inf to 2042.31836, saving model to model.h5\n",
      "15/15 [==============================] - 167s 11s/step - loss: 22720.6641 - accuracy: 0.1333 - val_loss: 2042.3184 - val_accuracy: 0.2000 - lr: 0.0010\n",
      "Epoch 2/70\n",
      "15/15 [==============================] - ETA: 0s - loss: 763.7047 - accuracy: 0.2120\n",
      "Epoch 2: val_loss improved from 2042.31836 to 18.61326, saving model to model.h5\n",
      "15/15 [==============================] - 156s 11s/step - loss: 763.7047 - accuracy: 0.2120 - val_loss: 18.6133 - val_accuracy: 0.2000 - lr: 0.0010\n",
      "Epoch 3/70\n",
      "15/15 [==============================] - ETA: 0s - loss: 12.8546 - accuracy: 0.2060\n",
      "Epoch 3: val_loss improved from 18.61326 to 3.84438, saving model to model.h5\n",
      "15/15 [==============================] - 156s 11s/step - loss: 12.8546 - accuracy: 0.2060 - val_loss: 3.8444 - val_accuracy: 0.1333 - lr: 0.0010\n",
      "Epoch 4/70\n",
      "15/15 [==============================] - ETA: 0s - loss: 5.5037 - accuracy: 0.1840\n",
      "Epoch 4: val_loss improved from 3.84438 to 2.11839, saving model to model.h5\n",
      "15/15 [==============================] - 155s 11s/step - loss: 5.5037 - accuracy: 0.1840 - val_loss: 2.1184 - val_accuracy: 0.2400 - lr: 0.0010\n",
      "Epoch 5/70\n",
      "15/15 [==============================] - ETA: 0s - loss: 2.9553 - accuracy: 0.1447\n",
      "Epoch 5: val_loss improved from 2.11839 to 1.59315, saving model to model.h5\n",
      "15/15 [==============================] - 156s 11s/step - loss: 2.9553 - accuracy: 0.1447 - val_loss: 1.5931 - val_accuracy: 0.1867 - lr: 0.0010\n",
      "Epoch 6/70\n",
      "15/15 [==============================] - ETA: 0s - loss: 1.7535 - accuracy: 0.2020\n",
      "Epoch 6: val_loss did not improve from 1.59315\n",
      "15/15 [==============================] - 97s 7s/step - loss: 1.7535 - accuracy: 0.2020 - val_loss: 1.7598 - val_accuracy: 0.2000 - lr: 0.0010\n",
      "Epoch 7/70\n",
      "15/15 [==============================] - ETA: 0s - loss: 1.6724 - accuracy: 0.2467\n",
      "Epoch 7: val_loss improved from 1.59315 to 1.56033, saving model to model.h5\n",
      "15/15 [==============================] - 155s 11s/step - loss: 1.6724 - accuracy: 0.2467 - val_loss: 1.5603 - val_accuracy: 0.2533 - lr: 0.0010\n",
      "Epoch 8/70\n",
      "15/15 [==============================] - ETA: 0s - loss: 1.6041 - accuracy: 0.3400\n",
      "Epoch 8: val_loss improved from 1.56033 to 1.51288, saving model to model.h5\n",
      "15/15 [==============================] - 159s 11s/step - loss: 1.6041 - accuracy: 0.3400 - val_loss: 1.5129 - val_accuracy: 0.2933 - lr: 0.0010\n",
      "Epoch 9/70\n",
      "15/15 [==============================] - ETA: 0s - loss: 1.5773 - accuracy: 0.3133\n",
      "Epoch 9: val_loss did not improve from 1.51288\n",
      "15/15 [==============================] - 98s 7s/step - loss: 1.5773 - accuracy: 0.3133 - val_loss: 1.6178 - val_accuracy: 0.2800 - lr: 0.0010\n",
      "Epoch 10/70\n",
      "15/15 [==============================] - ETA: 0s - loss: 1.5565 - accuracy: 0.3347\n",
      "Epoch 10: val_loss did not improve from 1.51288\n",
      "15/15 [==============================] - 96s 6s/step - loss: 1.5565 - accuracy: 0.3347 - val_loss: 1.5394 - val_accuracy: 0.2800 - lr: 0.0010\n",
      "Epoch 11/70\n",
      "15/15 [==============================] - ETA: 0s - loss: 1.5530 - accuracy: 0.3093\n",
      "Epoch 11: val_loss improved from 1.51288 to 1.50620, saving model to model.h5\n",
      "15/15 [==============================] - 152s 10s/step - loss: 1.5530 - accuracy: 0.3093 - val_loss: 1.5062 - val_accuracy: 0.2933 - lr: 0.0010\n",
      "Epoch 12/70\n",
      "15/15 [==============================] - ETA: 0s - loss: 1.5477 - accuracy: 0.3147\n",
      "Epoch 12: val_loss improved from 1.50620 to 1.48056, saving model to model.h5\n",
      "15/15 [==============================] - 154s 11s/step - loss: 1.5477 - accuracy: 0.3147 - val_loss: 1.4806 - val_accuracy: 0.3333 - lr: 0.0010\n",
      "Epoch 13/70\n",
      "15/15 [==============================] - ETA: 0s - loss: 1.5321 - accuracy: 0.3213\n",
      "Epoch 13: val_loss improved from 1.48056 to 1.46674, saving model to model.h5\n",
      "15/15 [==============================] - 154s 11s/step - loss: 1.5321 - accuracy: 0.3213 - val_loss: 1.4667 - val_accuracy: 0.3200 - lr: 0.0010\n",
      "Epoch 14/70\n",
      "15/15 [==============================] - ETA: 0s - loss: 1.5124 - accuracy: 0.3227\n",
      "Epoch 14: val_loss did not improve from 1.46674\n",
      "15/15 [==============================] - 96s 6s/step - loss: 1.5124 - accuracy: 0.3227 - val_loss: 1.4774 - val_accuracy: 0.2933 - lr: 0.0010\n",
      "Epoch 15/70\n",
      "15/15 [==============================] - ETA: 0s - loss: 1.5065 - accuracy: 0.3147\n",
      "Epoch 15: val_loss did not improve from 1.46674\n",
      "15/15 [==============================] - 96s 6s/step - loss: 1.5065 - accuracy: 0.3147 - val_loss: 1.4706 - val_accuracy: 0.2933 - lr: 0.0010\n",
      "Epoch 16/70\n",
      "15/15 [==============================] - ETA: 0s - loss: 1.4922 - accuracy: 0.3327\n",
      "Epoch 16: val_loss did not improve from 1.46674\n",
      "15/15 [==============================] - 95s 6s/step - loss: 1.4922 - accuracy: 0.3327 - val_loss: 1.4768 - val_accuracy: 0.3067 - lr: 0.0010\n",
      "Epoch 17/70\n",
      "15/15 [==============================] - ETA: 0s - loss: 1.4795 - accuracy: 0.3160\n",
      "Epoch 17: val_loss did not improve from 1.46674\n",
      "\n",
      "Epoch 17: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.\n",
      "15/15 [==============================] - 96s 6s/step - loss: 1.4795 - accuracy: 0.3160 - val_loss: 1.4881 - val_accuracy: 0.3200 - lr: 0.0010\n",
      "Epoch 18/70\n",
      "15/15 [==============================] - ETA: 0s - loss: 1.4708 - accuracy: 0.3233\n",
      "Epoch 18: val_loss did not improve from 1.46674\n",
      "15/15 [==============================] - 95s 6s/step - loss: 1.4708 - accuracy: 0.3233 - val_loss: 1.4897 - val_accuracy: 0.3200 - lr: 2.0000e-04\n",
      "Epoch 19/70\n",
      "15/15 [==============================] - ETA: 0s - loss: 1.4687 - accuracy: 0.3247\n",
      "Epoch 19: val_loss did not improve from 1.46674\n",
      "15/15 [==============================] - 97s 7s/step - loss: 1.4687 - accuracy: 0.3247 - val_loss: 1.4884 - val_accuracy: 0.3200 - lr: 2.0000e-04\n",
      "Epoch 20/70\n",
      "15/15 [==============================] - ETA: 0s - loss: 1.4659 - accuracy: 0.3233\n",
      "Epoch 20: val_loss did not improve from 1.46674\n",
      "15/15 [==============================] - 97s 6s/step - loss: 1.4659 - accuracy: 0.3233 - val_loss: 1.4863 - val_accuracy: 0.3200 - lr: 2.0000e-04\n",
      "Epoch 21/70\n",
      "15/15 [==============================] - ETA: 0s - loss: 1.4630 - accuracy: 0.3240\n",
      "Epoch 21: val_loss did not improve from 1.46674\n",
      "15/15 [==============================] - 97s 7s/step - loss: 1.4630 - accuracy: 0.3240 - val_loss: 1.4854 - val_accuracy: 0.3200 - lr: 2.0000e-04\n",
      "Epoch 22/70\n",
      "15/15 [==============================] - ETA: 0s - loss: 1.4601 - accuracy: 0.3293\n",
      "Epoch 22: val_loss did not improve from 1.46674\n",
      "\n",
      "Epoch 22: ReduceLROnPlateau reducing learning rate to 0.0001.\n",
      "15/15 [==============================] - 97s 6s/step - loss: 1.4601 - accuracy: 0.3293 - val_loss: 1.4855 - val_accuracy: 0.3200 - lr: 2.0000e-04\n",
      "Epoch 23/70\n",
      "15/15 [==============================] - ETA: 0s - loss: 1.4574 - accuracy: 0.3387\n",
      "Epoch 23: val_loss did not improve from 1.46674\n",
      "15/15 [==============================] - 97s 6s/step - loss: 1.4574 - accuracy: 0.3387 - val_loss: 1.4856 - val_accuracy: 0.3200 - lr: 1.0000e-04\n",
      "Epoch 24/70\n",
      "15/15 [==============================] - ETA: 0s - loss: 1.4556 - accuracy: 0.3387\n",
      "Epoch 24: val_loss did not improve from 1.46674\n",
      "15/15 [==============================] - 97s 7s/step - loss: 1.4556 - accuracy: 0.3387 - val_loss: 1.4862 - val_accuracy: 0.3200 - lr: 1.0000e-04\n",
      "Epoch 25/70\n",
      "15/15 [==============================] - ETA: 0s - loss: 1.4537 - accuracy: 0.3480\n",
      "Epoch 25: val_loss did not improve from 1.46674\n",
      "15/15 [==============================] - 97s 7s/step - loss: 1.4537 - accuracy: 0.3480 - val_loss: 1.4872 - val_accuracy: 0.3200 - lr: 1.0000e-04\n",
      "Epoch 26/70\n",
      "15/15 [==============================] - ETA: 0s - loss: 1.4519 - accuracy: 0.3500\n",
      "Epoch 26: val_loss did not improve from 1.46674\n",
      "15/15 [==============================] - 97s 6s/step - loss: 1.4519 - accuracy: 0.3500 - val_loss: 1.4882 - val_accuracy: 0.3067 - lr: 1.0000e-04\n",
      "Epoch 27/70\n",
      "15/15 [==============================] - ETA: 0s - loss: 1.4502 - accuracy: 0.3493\n",
      "Epoch 27: val_loss did not improve from 1.46674\n",
      "15/15 [==============================] - 98s 7s/step - loss: 1.4502 - accuracy: 0.3493 - val_loss: 1.4886 - val_accuracy: 0.3067 - lr: 1.0000e-04\n",
      "Epoch 28/70\n",
      "15/15 [==============================] - ETA: 0s - loss: 1.4483 - accuracy: 0.3473\n",
      "Epoch 28: val_loss did not improve from 1.46674\n",
      "15/15 [==============================] - 98s 7s/step - loss: 1.4483 - accuracy: 0.3473 - val_loss: 1.4886 - val_accuracy: 0.3067 - lr: 1.0000e-04\n",
      "Epoch 29/70\n",
      "15/15 [==============================] - ETA: 0s - loss: 1.4463 - accuracy: 0.3887\n",
      "Epoch 29: val_loss did not improve from 1.46674\n",
      "15/15 [==============================] - 98s 7s/step - loss: 1.4463 - accuracy: 0.3887 - val_loss: 1.4883 - val_accuracy: 0.3067 - lr: 1.0000e-04\n",
      "Epoch 30/70\n",
      "15/15 [==============================] - ETA: 0s - loss: 1.4443 - accuracy: 0.3893\n",
      "Epoch 30: val_loss did not improve from 1.46674\n",
      "15/15 [==============================] - 98s 7s/step - loss: 1.4443 - accuracy: 0.3893 - val_loss: 1.4877 - val_accuracy: 0.3067 - lr: 1.0000e-04\n",
      "Epoch 31/70\n",
      "15/15 [==============================] - ETA: 0s - loss: 1.4422 - accuracy: 0.3900\n",
      "Epoch 31: val_loss did not improve from 1.46674\n",
      "15/15 [==============================] - 98s 7s/step - loss: 1.4422 - accuracy: 0.3900 - val_loss: 1.4870 - val_accuracy: 0.3067 - lr: 1.0000e-04\n",
      "Epoch 32/70\n",
      "15/15 [==============================] - ETA: 0s - loss: 1.4401 - accuracy: 0.3920\n",
      "Epoch 32: val_loss did not improve from 1.46674\n",
      "15/15 [==============================] - 98s 7s/step - loss: 1.4401 - accuracy: 0.3920 - val_loss: 1.4864 - val_accuracy: 0.3067 - lr: 1.0000e-04\n",
      "Epoch 33/70\n",
      "15/15 [==============================] - ETA: 0s - loss: 1.4380 - accuracy: 0.3927\n",
      "Epoch 33: val_loss did not improve from 1.46674\n",
      "15/15 [==============================] - 99s 7s/step - loss: 1.4380 - accuracy: 0.3927 - val_loss: 1.4859 - val_accuracy: 0.3067 - lr: 1.0000e-04\n",
      "Epoch 34/70\n",
      "15/15 [==============================] - ETA: 0s - loss: 1.4358 - accuracy: 0.3940\n",
      "Epoch 34: val_loss did not improve from 1.46674\n",
      "15/15 [==============================] - 98s 7s/step - loss: 1.4358 - accuracy: 0.3940 - val_loss: 1.4855 - val_accuracy: 0.3067 - lr: 1.0000e-04\n",
      "Epoch 35/70\n",
      "15/15 [==============================] - ETA: 0s - loss: 1.4335 - accuracy: 0.3947\n",
      "Epoch 35: val_loss did not improve from 1.46674\n",
      "15/15 [==============================] - 98s 7s/step - loss: 1.4335 - accuracy: 0.3947 - val_loss: 1.4853 - val_accuracy: 0.3200 - lr: 1.0000e-04\n",
      "Epoch 36/70\n",
      "15/15 [==============================] - ETA: 0s - loss: 1.4313 - accuracy: 0.3973\n",
      "Epoch 36: val_loss did not improve from 1.46674\n",
      "15/15 [==============================] - 97s 6s/step - loss: 1.4313 - accuracy: 0.3973 - val_loss: 1.4851 - val_accuracy: 0.3200 - lr: 1.0000e-04\n",
      "Epoch 37/70\n",
      "15/15 [==============================] - ETA: 0s - loss: 1.4291 - accuracy: 0.4000\n",
      "Epoch 37: val_loss did not improve from 1.46674\n",
      "15/15 [==============================] - 97s 7s/step - loss: 1.4291 - accuracy: 0.4000 - val_loss: 1.4848 - val_accuracy: 0.3200 - lr: 1.0000e-04\n",
      "Epoch 38/70\n",
      "15/15 [==============================] - ETA: 0s - loss: 1.4268 - accuracy: 0.3993\n",
      "Epoch 38: val_loss did not improve from 1.46674\n",
      "15/15 [==============================] - 98s 7s/step - loss: 1.4268 - accuracy: 0.3993 - val_loss: 1.4846 - val_accuracy: 0.3200 - lr: 1.0000e-04\n",
      "Epoch 39/70\n",
      "15/15 [==============================] - ETA: 0s - loss: 1.4245 - accuracy: 0.4007\n",
      "Epoch 39: val_loss did not improve from 1.46674\n",
      "15/15 [==============================] - 98s 7s/step - loss: 1.4245 - accuracy: 0.4007 - val_loss: 1.4846 - val_accuracy: 0.3200 - lr: 1.0000e-04\n",
      "Epoch 40/70\n",
      "15/15 [==============================] - ETA: 0s - loss: 1.4222 - accuracy: 0.4007\n",
      "Epoch 40: val_loss did not improve from 1.46674\n",
      "15/15 [==============================] - 98s 7s/step - loss: 1.4222 - accuracy: 0.4007 - val_loss: 1.4850 - val_accuracy: 0.3200 - lr: 1.0000e-04\n",
      "Epoch 41/70\n",
      "15/15 [==============================] - ETA: 0s - loss: 1.4198 - accuracy: 0.4007\n",
      "Epoch 41: val_loss did not improve from 1.46674\n",
      "15/15 [==============================] - 99s 7s/step - loss: 1.4198 - accuracy: 0.4007 - val_loss: 1.4861 - val_accuracy: 0.3200 - lr: 1.0000e-04\n",
      "Epoch 42/70\n",
      "15/15 [==============================] - ETA: 0s - loss: 1.4174 - accuracy: 0.3987\n",
      "Epoch 42: val_loss did not improve from 1.46674\n",
      "15/15 [==============================] - 97s 7s/step - loss: 1.4174 - accuracy: 0.3987 - val_loss: 1.4876 - val_accuracy: 0.3200 - lr: 1.0000e-04\n",
      "Epoch 43/70\n",
      "15/15 [==============================] - ETA: 0s - loss: 1.4149 - accuracy: 0.3953\n",
      "Epoch 43: val_loss did not improve from 1.46674\n",
      "15/15 [==============================] - 98s 7s/step - loss: 1.4149 - accuracy: 0.3953 - val_loss: 1.4894 - val_accuracy: 0.3200 - lr: 1.0000e-04\n",
      "Epoch 44/70\n",
      "15/15 [==============================] - ETA: 0s - loss: 1.4125 - accuracy: 0.3960\n",
      "Epoch 44: val_loss did not improve from 1.46674\n",
      "15/15 [==============================] - 98s 7s/step - loss: 1.4125 - accuracy: 0.3960 - val_loss: 1.4914 - val_accuracy: 0.3200 - lr: 1.0000e-04\n",
      "Epoch 45/70\n",
      "15/15 [==============================] - ETA: 0s - loss: 1.4100 - accuracy: 0.3953\n",
      "Epoch 45: val_loss did not improve from 1.46674\n",
      "15/15 [==============================] - 97s 6s/step - loss: 1.4100 - accuracy: 0.3953 - val_loss: 1.4933 - val_accuracy: 0.3200 - lr: 1.0000e-04\n",
      "Epoch 46/70\n",
      "15/15 [==============================] - ETA: 0s - loss: 1.4075 - accuracy: 0.3947\n",
      "Epoch 46: val_loss did not improve from 1.46674\n",
      "15/15 [==============================] - 98s 7s/step - loss: 1.4075 - accuracy: 0.3947 - val_loss: 1.4950 - val_accuracy: 0.3200 - lr: 1.0000e-04\n",
      "Epoch 47/70\n",
      "15/15 [==============================] - ETA: 0s - loss: 1.4049 - accuracy: 0.3993\n",
      "Epoch 47: val_loss did not improve from 1.46674\n",
      "15/15 [==============================] - 97s 6s/step - loss: 1.4049 - accuracy: 0.3993 - val_loss: 1.4964 - val_accuracy: 0.3200 - lr: 1.0000e-04\n",
      "Epoch 48/70\n",
      "15/15 [==============================] - ETA: 0s - loss: 1.4023 - accuracy: 0.4013\n",
      "Epoch 48: val_loss did not improve from 1.46674\n",
      "15/15 [==============================] - 96s 6s/step - loss: 1.4023 - accuracy: 0.4013 - val_loss: 1.4978 - val_accuracy: 0.3200 - lr: 1.0000e-04\n",
      "Epoch 49/70\n",
      "15/15 [==============================] - ETA: 0s - loss: 1.3997 - accuracy: 0.4007\n",
      "Epoch 49: val_loss did not improve from 1.46674\n",
      "15/15 [==============================] - 97s 6s/step - loss: 1.3997 - accuracy: 0.4007 - val_loss: 1.4995 - val_accuracy: 0.3200 - lr: 1.0000e-04\n",
      "Epoch 50/70\n",
      "15/15 [==============================] - ETA: 0s - loss: 1.3972 - accuracy: 0.3993\n",
      "Epoch 50: val_loss did not improve from 1.46674\n",
      "15/15 [==============================] - 97s 6s/step - loss: 1.3972 - accuracy: 0.3993 - val_loss: 1.5015 - val_accuracy: 0.3200 - lr: 1.0000e-04\n",
      "Epoch 51/70\n",
      "15/15 [==============================] - ETA: 0s - loss: 1.3947 - accuracy: 0.3993\n",
      "Epoch 51: val_loss did not improve from 1.46674\n",
      "15/15 [==============================] - 98s 7s/step - loss: 1.3947 - accuracy: 0.3993 - val_loss: 1.5039 - val_accuracy: 0.3200 - lr: 1.0000e-04\n",
      "Epoch 52/70\n",
      "15/15 [==============================] - ETA: 0s - loss: 1.3922 - accuracy: 0.4000\n",
      "Epoch 52: val_loss did not improve from 1.46674\n",
      "15/15 [==============================] - 97s 7s/step - loss: 1.3922 - accuracy: 0.4000 - val_loss: 1.5068 - val_accuracy: 0.3200 - lr: 1.0000e-04\n",
      "Epoch 53/70\n",
      "15/15 [==============================] - ETA: 0s - loss: 1.3897 - accuracy: 0.4040\n",
      "Epoch 53: val_loss did not improve from 1.46674\n",
      "15/15 [==============================] - 97s 6s/step - loss: 1.3897 - accuracy: 0.4040 - val_loss: 1.5100 - val_accuracy: 0.3200 - lr: 1.0000e-04\n",
      "Epoch 54/70\n",
      "15/15 [==============================] - ETA: 0s - loss: 1.3872 - accuracy: 0.4027\n",
      "Epoch 54: val_loss did not improve from 1.46674\n",
      "15/15 [==============================] - 98s 7s/step - loss: 1.3872 - accuracy: 0.4027 - val_loss: 1.5133 - val_accuracy: 0.3200 - lr: 1.0000e-04\n",
      "Epoch 55/70\n",
      "15/15 [==============================] - ETA: 0s - loss: 1.3846 - accuracy: 0.4027\n",
      "Epoch 55: val_loss did not improve from 1.46674\n",
      "15/15 [==============================] - 98s 7s/step - loss: 1.3846 - accuracy: 0.4027 - val_loss: 1.5170 - val_accuracy: 0.3200 - lr: 1.0000e-04\n",
      "Epoch 56/70\n",
      "15/15 [==============================] - ETA: 0s - loss: 1.3821 - accuracy: 0.4033\n",
      "Epoch 56: val_loss did not improve from 1.46674\n",
      "15/15 [==============================] - 97s 6s/step - loss: 1.3821 - accuracy: 0.4033 - val_loss: 1.5209 - val_accuracy: 0.3200 - lr: 1.0000e-04\n",
      "Epoch 57/70\n",
      "15/15 [==============================] - ETA: 0s - loss: 1.3796 - accuracy: 0.4047\n",
      "Epoch 57: val_loss did not improve from 1.46674\n",
      "15/15 [==============================] - 97s 7s/step - loss: 1.3796 - accuracy: 0.4047 - val_loss: 1.5247 - val_accuracy: 0.3200 - lr: 1.0000e-04\n",
      "Epoch 58/70\n",
      "15/15 [==============================] - ETA: 0s - loss: 1.3772 - accuracy: 0.4053\n",
      "Epoch 58: val_loss did not improve from 1.46674\n",
      "15/15 [==============================] - 97s 6s/step - loss: 1.3772 - accuracy: 0.4053 - val_loss: 1.5289 - val_accuracy: 0.3200 - lr: 1.0000e-04\n",
      "Epoch 59/70\n",
      "15/15 [==============================] - ETA: 0s - loss: 1.3748 - accuracy: 0.4047WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 5 batches). You may need to use the repeat() function when building your dataset.\n",
      "\n",
      "Epoch 59: val_loss did not improve from 1.46674\n",
      "15/15 [==============================] - 98s 7s/step - loss: 1.3748 - accuracy: 0.4047 - val_loss: 1.5049 - val_accuracy: 0.5333 - lr: 1.0000e-04\n",
      "Epoch 60/70\n",
      "15/15 [==============================] - ETA: 0s - loss: 1.3724 - accuracy: 0.4020WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_accuracy` which is not available. Available metrics are: loss,accuracy,lr\n",
      "15/15 [==============================] - 96s 6s/step - loss: 1.3724 - accuracy: 0.4020 - lr: 1.0000e-04\n",
      "Epoch 61/70\n",
      "15/15 [==============================] - ETA: 0s - loss: 1.3701 - accuracy: 0.4060WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_accuracy` which is not available. Available metrics are: loss,accuracy,lr\n",
      "15/15 [==============================] - 98s 7s/step - loss: 1.3701 - accuracy: 0.4060 - lr: 1.0000e-04\n",
      "Epoch 62/70\n",
      "15/15 [==============================] - ETA: 0s - loss: 1.3678 - accuracy: 0.4093WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_accuracy` which is not available. Available metrics are: loss,accuracy,lr\n",
      "15/15 [==============================] - 93s 6s/step - loss: 1.3678 - accuracy: 0.4093 - lr: 1.0000e-04\n",
      "Epoch 63/70\n",
      "15/15 [==============================] - ETA: 0s - loss: 1.3657 - accuracy: 0.4093WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_accuracy` which is not available. Available metrics are: loss,accuracy,lr\n",
      "15/15 [==============================] - 97s 6s/step - loss: 1.3657 - accuracy: 0.4093 - lr: 1.0000e-04\n",
      "Epoch 64/70\n",
      "15/15 [==============================] - ETA: 0s - loss: 1.3635 - accuracy: 0.4087WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_accuracy` which is not available. Available metrics are: loss,accuracy,lr\n",
      "15/15 [==============================] - 93s 6s/step - loss: 1.3635 - accuracy: 0.4087 - lr: 1.0000e-04\n",
      "Epoch 65/70\n",
      "15/15 [==============================] - ETA: 0s - loss: 1.3612 - accuracy: 0.4093WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_accuracy` which is not available. Available metrics are: loss,accuracy,lr\n",
      "15/15 [==============================] - 93s 6s/step - loss: 1.3612 - accuracy: 0.4093 - lr: 1.0000e-04\n",
      "Epoch 66/70\n",
      "15/15 [==============================] - ETA: 0s - loss: 1.3588 - accuracy: 0.4100WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_accuracy` which is not available. Available metrics are: loss,accuracy,lr\n",
      "15/15 [==============================] - 97s 6s/step - loss: 1.3588 - accuracy: 0.4100 - lr: 1.0000e-04\n",
      "Epoch 67/70\n",
      "15/15 [==============================] - ETA: 0s - loss: 1.3561 - accuracy: 0.4133WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_accuracy` which is not available. Available metrics are: loss,accuracy,lr\n",
      "15/15 [==============================] - 95s 6s/step - loss: 1.3561 - accuracy: 0.4133 - lr: 1.0000e-04\n",
      "Epoch 68/70\n",
      "15/15 [==============================] - ETA: 0s - loss: 1.3531 - accuracy: 0.4113WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_accuracy` which is not available. Available metrics are: loss,accuracy,lr\n",
      "15/15 [==============================] - 94s 6s/step - loss: 1.3531 - accuracy: 0.4113 - lr: 1.0000e-04\n",
      "Epoch 69/70\n",
      "15/15 [==============================] - ETA: 0s - loss: 1.3500 - accuracy: 0.4120WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_accuracy` which is not available. Available metrics are: loss,accuracy,lr\n",
      "15/15 [==============================] - 94s 6s/step - loss: 1.3500 - accuracy: 0.4120 - lr: 1.0000e-04\n",
      "Epoch 70/70\n",
      "14/15 [===========================>..] - ETA: 6s - loss: 1.3437 - accuracy: 0.4221 WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 1050 batches). You may need to use the repeat() function when building your dataset.\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_accuracy` which is not available. Available metrics are: loss,accuracy,lr\n",
      "15/15 [==============================] - 88s 6s/step - loss: 1.3437 - accuracy: 0.4221 - lr: 1.0000e-04\n"
     ]
    }
   ],
   "source": [
    "# Callbacks can be used to stop early, decrease learning rate, checkpoint the model, etc.\n",
    "#from keras.callbacks import EarlyStopping\n",
    "#stopping  = keras.callbacks.EarlyStopping(monitor='val_loss', patience=0, verbose=0, mode='auto')\n",
    "# callbacks = []#[stopping]\n",
    "\n",
    "# The fit_generator function loads data batches on the fly, instead of transfering entire data set to the gpu.\n",
    "history   = model.fit(data_train,\n",
    "                        steps_per_epoch=int(1500/100),\n",
    "                        epochs=70, verbose=1,\n",
    "                        callbacks=callbacks, \n",
    "                        validation_data=data_valid,\n",
    "                        validation_steps=int(75/15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x1f807a53220>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot loss trajectory throughout training.\n",
    "plt.figure(1, figsize=(10,5))\n",
    "plt.plot(history.history['accuracy'], label='primary')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.models import load_model\n",
    "from PIL import Image\n",
    "\n",
    "# Load the saved model.\n",
    "model = load_model('model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(800, 600)\n",
      "1/1 [==============================] - 0s 270ms/step\n",
      "Predicted Class:  1\n"
     ]
    }
   ],
   "source": [
    "inputs = {}\n",
    "img1 = np.transpose(np.asarray(Image.open(\"data/test_bw/up/up_0.jpg\").convert('L')))\n",
    "# img2 = np.expand_dims(np.transpose(np.asarray(Image.open(\"test_bw/x/x_2.jpg\").convert('L'))), axis=-1)\n",
    "print(img1.shape)\n",
    "\n",
    "inputs['input1'] = np.reshape(img1, (1, 800, 600, 1))\n",
    "inputs['input2'] = np.reshape(img1, (1, 800, 600, 1))\n",
    "# inputs['input2'] = np.reshape(img2, (1, 800, 600, 1))\n",
    "\n",
    "\n",
    "predictions = model.predict(inputs)\n",
    "print(\"Predicted Class: \", np.argmax(predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs, outputs = data_valid.__next__()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
